# **GraphSeeker**

### *Scalable Graph Retrieval with Siamese Graph Transformers and GNN-Based Reranking*

[![Python Version](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Built with PyTorch](https://img.shields.io/badge/Built%20with-PyTorch-EE4C2C?logo=pytorch)](https://pytorch.org/)
[![PyTorch Geometric](https://img.shields.io/badge/PyTorch%20Geometric-Framework-brightgreen?logo=github)](https://pytorch-geometric.readthedocs.io/)
[![Dataset](https://img.shields.io/badge/Dataset-ZINC-lightblue)](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.ZINC)
[![Project Status](https://img.shields.io/badge/Status-Actively%20Developed-success)]()

---

**GraphSeeker** is a high-performance **graph retrieval framework** built on **Siamese Graph Transformers** and a **two-stage GNN-based reranking pipeline**. It is designed for applications in:

-  Molecular similarity
-  Semantic graph search
-  Graph clustering

with a focus on **scalability**, **generalizability**, and **robust contrastive learning**.

---

## ğŸ§ª Contrastive Pretraining with Augmentations

We employ **in-batch contrastive learning** with the **NT-Xent loss**, inspired by SimCLR.

**Training Strategy:**

- Every graph `G` is paired with a stochastically augmented copy â†’ `(Gâ‚, Gâ‚‚)`
- These form **positive pairs**; all others in the batch are treated as **negatives**
- Augmentations used:
  - **DropEdge** â€” Randomly removes edges
  - **FeatureMasking** â€” Masks node features

> This strategy eliminates the need for manual positive/negative sampling and supports out-of-the-box generalization across datasets.

---

## âœ¨ Model Overview

### ğŸ§  Siamese Graph Transformer Encoder

- **Edge-Conditioned Attention**: Injects edge features directly into the attention weights
- **Global Token**: Learnable node for graph-level representation
- **Augmentation-Aware Training**: Learns structure-invariant representations
- **Task-Agnostic Embeddings**: Suitable for retrieval, clustering, and few-shot tasks

---

## ğŸ” Two-Stage Retrieval Pipeline

### ğŸš€ Stage 1: FAISS-Based ANN Retrieval

- **Fast approximate nearest neighbor** retrieval
- Based on **IVF+PQ indexing**
- Tunable parameters for accuracy-speed tradeoff: `NLIST`, `NBITS`, `NPROBE`

### ğŸ¤– Stage 2: GNN-Based Reranker

- Constructs a **supergraph** from the query and retrieved candidates
- Uses a **cross-encoder GNN** for joint representation learning
- Trained with contrastive supervision (extendable to true relevance labels)

---

## ğŸ§± System Architecture

```mermaid
graph TD
    A[Query Graph] --> B[Siamese Graph Transformer Encoder]
    B --> C[Graph-Level Embedding]
    C --> D[FAISS IVF+PQ ANN Retrieval]
    D --> E[Top-K Candidate Graphs]
    E --> F[Cross-Encoder GNN Reranker]
    F --> G[Final Ranked Results]
````

---

## ğŸ“‰ Training Loss Curve

<p align="center">
  <img src="assets/loss_curve_final.png" alt="Training Loss Curve" width="600"/>
</p>

The NT-Xent loss **steadily decreases**, indicating stable convergence during contrastive pretraining.

---

## ğŸ“ˆ Positive Similarity Curve

<p align="center">
  <img src="assets/pos_sim.png" alt="Positive Similarity Curve" width="600"/>
</p>

The average cosine similarity between positive graph pairs **increased consistently during training**, eventually reaching a value of **0.96**, indicating highly aligned latent embeddings for augmented versions of the same graph.

---

## ğŸ“ Project Structure

```
GraphSeeker/
â”œâ”€â”€ Siamese-Graphormer/
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ dataset.py
â”‚   â”œâ”€â”€ loss/
â”‚   â”‚   â””â”€â”€ loss.py
â”‚   â””â”€â”€ network/
â”‚       â”œâ”€â”€ siamese.py
â”‚       â”œâ”€â”€ encoder.py
â”‚       â””â”€â”€ edge_attention.py
â”œâ”€â”€ Graph_Retriever/
â”‚   â”œâ”€â”€ get_similiar.py
â”‚   â”œâ”€â”€ train_re_ranker.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ config.yaml
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â”œâ”€â”€ check.ipynb
â”‚   â”‚   â””â”€â”€ graph_embeddings.pt
â”‚   â””â”€â”€ network/
â”‚       â”œâ”€â”€ hybrid_retrieval.py
â”‚       â””â”€â”€ re_ranker.py
â””â”€â”€ assets/
    â”œâ”€â”€ loss_curve.png
    â””â”€â”€ pos_sim_curve.png
```

---

## âš™ï¸ Quickstart

### 1ï¸âƒ£ Pretrain the Siamese Graph Transformer

```bash
cd Siamese-Graphormer
python train.py
```

* Uses **in-batch NT-Xent loss**
* Applies **DropEdge** and **FeatureMasking**
* Saves embeddings to `embeddings/train_graph_embeddings.npy`
* Automatically uses **GPU if available**

### 2ï¸âƒ£ Run Hybrid Retrieval

```bash
cd ../Graph_Retriever
python get_similiar.py
```

* Retrieves top-k candidates via FAISS
* Reranks them using the GNN cross-encoder

---

## ğŸ“Š Dataset: ZINC

* Provided by **PyTorch Geometric**
* Each graph represents a molecule with:

  * Node features: atom types
  * Edge features: bond types
* Augmentations are applied **on-the-fly** during training

---

## ğŸ“ˆ Training Logs

We log the following metrics:

* ğŸ“‰ NT-Xent **contrastive loss**
* ğŸ“ˆ Average **positive similarity** (final value: **0.96**)

For deeper monitoring, integrate with **TensorBoard** or **Weights & Biases**.

---

## ğŸ¤ Contributions Welcome

We're actively expanding this framework!

### Ideas for contribution:

* ğŸ“¦ Add support for **larger molecular datasets**
* âš¡ Speed up reranking with lightweight GNNs
* ğŸ” Add **text-conditioned graph retrieval**
* ğŸ“Š Benchmark on **OGBG**, **QM9**, **PCQM4M**, etc.

---

## ğŸ“œ License

Distributed under the **MIT License**. See `LICENSE` for details.

